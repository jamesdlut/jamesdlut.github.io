---
typora-copy-images-to: 深度学习代码截图
---



# 目录



[TOC]



* 这其实也是在攒着自己的utils.py

### 0 数据预备函数

###### transform 各种版本

###### 1 李沐120版 2个transform 96 96 train比test做的增强要多

auglist的写法 比下面的抽象更高一些

```
def transform_train(data, label):
    im = image.imresize(data.astype('float32') / 255, 96, 96)
    auglist = image.CreateAugmenter(data_shape=(3, 96, 96), resize=0,
                        rand_crop=False, rand_resize=False, rand_mirror=True,
                        mean=None, std=None,
                        brightness=0, contrast=0,
                        saturation=0, hue=0,
                        pca_noise=0, rand_gray=0, inter_method=2)
    for aug in auglist:
        im = aug(im)
    # 将数据格式从"高*宽*通道"改为"通道*高*宽"。
    im = nd.transpose(im, (2,0,1))
    return (im, nd.array([label]).asscalar().astype('float32'))

def transform_test(data, label):
    im = image.imresize(data.astype('float32') / 255, 96, 96)
    im = nd.transpose(im, (2,0,1))
    return (im, nd.array([label]).asscalar().astype('float32'))
```

###### 2 李沐 三参套牌版一个transfrom，augs单独提出来

其实api就要求的必须是两个元素参数版本的transform

自己手写的auglist,上面是自动生成的list

```
train_augs = [
    image.HorizontalFlipAug(.5),
    image.RandomCropAug((224,224))
]

test_augs = [
    image.CenterCropAug((224,224))
]
```

augs 是list

```
def transform(data, label, augs):
    data = data.astype('float32')
    for aug in augs:
        data = aug(data)
    data = nd.transpose(data, (2,0,1))
    return data, nd.array([label]).asscalar().astype('float32')
```

最后还是两个参数

```python
train_imgs = gluon.data.vision.ImageFolderDataset(
    data_dir+'/hotdog/train',
    transform=lambda X, y: transform(X, y, train_augs))
test_imgs = gluon.data.vision.ImageFolderDataset(
    data_dir+'/hotdog/test',
    transform=lambda X, y: transform(X, y, test_augs))
```



###### 3 ypw版 一个transfrom model zoo augs也在外 但不作为参数 

做不做为参数，要和import from import考虑起来

```
preprocessing = [
    image.ForceResizeAug((224,224)),
    image.ColorNormalizeAug(mean=nd.array([0.485, 0.456, 0.406]), std=nd.array([0.229, 0.224, 0.225]))
]

def transform(data, label):
    data = data.astype('float32') / 255
    for pre in preprocessing:
        data = pre(data)
    
    data = nd.transpose(data, (2,0,1))
    return data, nd.array([label]).asscalar().astype('float32')
```

transform 函数写干净，不同的操作在augs(pres) [] 里面体现

###### 4_基于2,3 多种pres，lambda

把transform_pres固定住，变化体现在pres[]中

```
preprocessing_224 = [
    image.ForceResizeAug((224,224)),
    image.ColorNormalizeAug(mean=nd.array([0.485, 0.456, 0.406]), std=nd.array([0.229, 0.224, 0.225]))
]

preprocessing_299 = [
    image.ForceResizeAug((299,299)),
    image.ColorNormalizeAug(mean=nd.array([0.485, 0.456, 0.406]), std=nd.array([0.229, 0.224, 0.225]))
]

def transform_pres(data, label,pres):
    data = data.astype('float32') / 255
    for pre in pres:
        data = pre(data)
    
    data = nd.transpose(data, (2,0,1))
    return data, nd.array([label]).asscalar().astype('float32')
```

```
transform=lambda X, y: transform_presX, y, preprocessing_224))

transform=lambda X, y: transform_presX, y, preprocessing_299))
```

![1513303867455](深度学习代码截图/1513303867455.png)



#### 1 dataset

#### 2 dataloader

#### 3 数据并行 多卡

![1513305500385](深度学习代码截图/1513305500385.png)



![1513305524282](深度学习代码截图/1513305524282.png)



![1513305588588](深度学习代码截图/1513305588588.png)

### 2 模型

### 3 损失

### 4 优化

### 5  训练

##### gpu的使用

![1513311208093](深度学习代码截图/1513311208093.png)

net 和 数据必须在同一设备上！否则会 list out of range报错！！

##### 各种训练函数

###### 多gpu

```
def train(num_gpus, batch_size, lr):
    train_data, test_data = utils.load_data_fashion_mnist(batch_size)
	
    ctx = [gpu(i) for i in range(num_gpus)] ####################################
    
    print('Running on', ctx)

    net = utils.resnet18(10)
    net.initialize(init=init.Xavier(), ctx=ctx)
    loss = gluon.loss.SoftmaxCrossEntropyLoss()
    trainer = gluon.Trainer(
        net.collect_params(),'sgd', {'learning_rate': lr})

    for epoch in range(5):
        start = time()
        total_loss = 0
        for data, label in train_data:
            data_list = gluon.utils.split_and_load(data, ctx) ######################
            label_list = gluon.utils.split_and_load(label, ctx)
            with autograd.record():
                losses = [loss(net(X), y) for X, y in zip(
                    data_list, label_list)]
            for l in losses:
                l.backward()
            total_loss += sum([l.sum().asscalar() for l in losses])
            trainer.step(batch_size)

        nd.waitall()
        print('Epoch %d, training time = %.1f sec'%(
            epoch, time()-start))

        test_acc = utils.evaluate_accuracy(test_data, net, ctx[0])
        print('         validation accuracy = %.4f'%(test_acc))
```

![1513306442114](深度学习代码截图/1513306442114.png)

